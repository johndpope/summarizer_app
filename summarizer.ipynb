{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.0 Import modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING (theano.configdefaults): g++ not detected ! Theano will be unable to execute optimized C-implementations (for both CPU and GPU) and will default to Python implementations. Performance will be severely degraded. To remove this warning, set Theano flags cxx to an empty string.\n"
     ]
    }
   ],
   "source": [
    "## NLP modules\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec\n",
    "import nltk\n",
    "import textblob\n",
    "from textblob import TextBlob\n",
    "from sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\n",
    "from nltk.corpus import stopwords ##Note you'll need to download NLTK and corpuses\n",
    "from spacy.en import English ##Note you'll need to install Spacy and download its dependencies\n",
    "parser = English()\n",
    "\n",
    "## Other Python modules\n",
    "import itertools\n",
    "from operator import itemgetter\n",
    "import re\n",
    "import string\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "%matplotlib inline\n",
    "\n",
    "## Graph module\n",
    "import networkx as nx\n",
    "\n",
    "## Machine learning & text vectorizer modules\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.0 Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 Text pre-processing functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# A custom function to clean the text before sending it into the vectorizer\n",
    "def cleanText(text):\n",
    "    # get rid of newlines\n",
    "    text = text.strip().replace(\"\\n\", \" \").replace(\"\\r\", \" \")\n",
    "    \n",
    "    # replace twitter @mentions\n",
    "    mentionFinder = re.compile(r\"@[a-z0-9_]{1,15}\", re.IGNORECASE)\n",
    "    text = mentionFinder.sub(\"@MENTION\", text)\n",
    "    text = re.sub('[^a-zA-Z ]','',text)\n",
    "    # replace HTML symbols\n",
    "    text = text.replace(\"&amp;\", \"and\").replace(\"&gt;\", \">\").replace(\"&lt;\", \"<\")\n",
    "    \n",
    "    # lowercase\n",
    "    text = text.lower()\n",
    "#     text = str(TextBlob(text).correct())\n",
    "    return text\n",
    "\n",
    "# A custom function to tokenize the text using spaCy\n",
    "# and convert to lemmas\n",
    "def tokenizeText(sample):\n",
    "    # get the tokens using spaCy\n",
    "    tokens = parser(sample)\n",
    "\n",
    "    # lemmatize\n",
    "    lemmas = []\n",
    "    for tok in tokens:\n",
    "        lemmas.append(tok.lemma_.lower().strip() if tok.lemma_ != \"-PRON-\" else tok.lower_)\n",
    "    tokens = lemmas\n",
    "\n",
    "    # stoplist the tokens\n",
    "    tokens = [tok for tok in tokens if tok not in STOPLIST]\n",
    "\n",
    "    # stoplist symbols\n",
    "    tokens = [tok for tok in tokens if tok not in SYMBOLS]\n",
    "\n",
    "    # remove large strings of whitespace\n",
    "    while \"\" in tokens:\n",
    "        tokens.remove(\"\")\n",
    "    while \" \" in tokens:\n",
    "        tokens.remove(\" \")\n",
    "    while \"\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\")\n",
    "    while \"\\n\\n\" in tokens:\n",
    "        tokens.remove(\"\\n\\n\")\n",
    "\n",
    "    return tokens\n",
    "\n",
    "# A custom stoplist\n",
    "STOPLIST = set(stopwords.words('english') + [\"n't\", \"'s\", \"'m\", \"ca\"] + list(ENGLISH_STOP_WORDS))\n",
    "# List of symbols we don't care about\n",
    "SYMBOLS = \" \".join(string.punctuation).split(\" \") + [\"-----\", \"---\", \"...\", \"“\", \"”\", \"'ve\"]\n",
    "\n",
    "## Tokenizer specific for Doc2Vec where each word is important, so stop words are not removed.\n",
    "def doc2vec_tokenizeText(corpus):\n",
    "    punctuation = \"\"\".,?!:;(){}[]\"\"\"\n",
    "    corpus = [z.lower().replace('\\n','') for z in corpus]\n",
    "    corpus = [z.replace('<br />', ' ') for z in corpus]\n",
    "\n",
    "    #treat punctuation as individual words\n",
    "    for c in punctuation:\n",
    "        corpus = [z.replace(c, ' %s '%c) for z in corpus]\n",
    "    corpus = [z.split() for z in corpus]\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 Similarity measure functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def cos_sim(text1,text2):\n",
    "    tfvectorizer = TfidfVectorizer(tokenizer=tokenizeText)\n",
    "    arrays = tfvectorizer.fit_transform([text1,text2]).A\n",
    "    num = (arrays[0]*arrays[1]).sum()\n",
    "    denom1 = np.sqrt((arrays[0]**2).sum())\n",
    "    denom2 = np.sqrt((arrays[1]**2).sum())\n",
    "    return num/(denom1*denom2)\n",
    "\n",
    "def similarity(string1,string2):\n",
    "    w1 = tokenizeText(cleanText(string1))\n",
    "    w2 = tokenizeText(cleanText(string2))\n",
    "    score = 0\n",
    "    for w in w1:\n",
    "        if w in w2:\n",
    "            score += 1\n",
    "        else:\n",
    "            continue\n",
    "    for w in w2:\n",
    "        if w in w1:\n",
    "            score += 1\n",
    "        else:\n",
    "            continue\n",
    "    return score/(len(w1)+len(w2))\n",
    "\n",
    "def lDistance(firstString, secondString):\n",
    "    \"Function to find the Levenshtein distance between two words/sentences - gotten from http://rosettacode.org/wiki/Levenshtein_distance#Python\"\n",
    "    if len(firstString) > len(secondString):\n",
    "        firstString, secondString = secondString, firstString\n",
    "    distances = range(len(firstString) + 1)\n",
    "    for index2, char2 in enumerate(secondString):\n",
    "        newDistances = [index2 + 1]\n",
    "        for index1, char1 in enumerate(firstString):\n",
    "            if char1 == char2:\n",
    "                newDistances.append(distances[index1])\n",
    "            else:\n",
    "                newDistances.append(1 + min((distances[index1], distances[index1+1], newDistances[-1])))\n",
    "        distances = newDistances\n",
    "    return 1/(distances[-1]+1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 Graph building functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def buildGraph(nodes,weight):\n",
    "    \"nodes - list of hashables that represents the nodes of the graph\"\n",
    "    gr = nx.Graph() #initialize an undirected graph\n",
    "    gr.add_nodes_from(nodes)\n",
    "    nodePairs = list(itertools.combinations(nodes, 2))\n",
    "\n",
    "    #add edges to the graph (weighted by Levenshtein distance)\n",
    "    for pair in nodePairs:\n",
    "        firstString = pair[0]\n",
    "        secondString = pair[1]\n",
    "        if weight == 'cosine':\n",
    "            edge_weight = cos_sim(firstString, secondString)\n",
    "        if weight == 'similarity':\n",
    "            edge_weight = similarity(firstString, secondString)\n",
    "        if weight == 'ldistance':\n",
    "            edge_weight = lDistance(firstString, secondString)\n",
    "        gr.add_edge(firstString, secondString, weight=edge_weight)\n",
    "    \n",
    "    return gr\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 Summarizer functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "def draw_graph(text,weight='cosine'):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "    graph = buildGraph(sentenceTokens,weight=weight)\n",
    "    nx.draw_networkx(graph)\n",
    "\n",
    "def textrank_summarizer(text,weight='cosine',num_sen = 5, pos_score = False):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "    graph = buildGraph(sentenceTokens,weight=weight)\n",
    "\n",
    "    calculated_page_rank = nx.pagerank(graph, weight='weight')\n",
    "    \n",
    "    #Create position scorelist\n",
    "    #Scores are caluclated such that page rank score is increased by 10% if its the first or last sentence\n",
    "    #Sentences in the middle of the document are not given increased scores\n",
    "    pos = np.array(list(range(len(sentenceTokens))))\n",
    "#     score_array = np.array(1+(abs((pos+0.5) - len(pos)/2)/max((pos+0.5) - len(pos)/2)/10))\n",
    "#     score_dict = {}\n",
    "#     for i in range(len(sentenceTokens)):\n",
    "#         score_dict[sentenceTokens[i]] = score_array[i] \n",
    "\n",
    "#     #Adjusts page rank score for position\n",
    "#     score_adj_page_rank = {k : v * score_dict[k] for k, v in calculated_page_rank.items() if k in score_dict}\n",
    "    \n",
    "    #most important sentences in ascending order of importance\n",
    "#     if pos_score == True:\n",
    "#         sentences = sorted(score_adj_page_rank, key=score_adj_page_rank.get,reverse=True)\n",
    "#     else:\n",
    "#         sentences = sorted(calculated_page_rank, key=calculated_page_rank.get,reverse=True)\n",
    "  \n",
    "    #return a word summary\n",
    "    pos_dict = {}\n",
    "    for i in range(len(sentenceTokens)):\n",
    "        pos_dict[sentenceTokens[i]] = pos[i] \n",
    "\n",
    "    combined = {k : [v, pos_dict[k]] for k, v in calculated_page_rank.items() if k in pos_dict}\n",
    "\n",
    "    listlist = []\n",
    "    for k, v in combined.items():\n",
    "        listlist.append((k,v[0],v[1]))\n",
    "\n",
    "    listlist.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    summarysentences = listlist[0:num_sen]\n",
    "\n",
    "    summarysentences.sort(key=lambda x: x[2],reverse=False)\n",
    "\n",
    "    summary = \"\"\n",
    "    for n in range(num_sen):\n",
    "        summary += ' ' + summarysentences[n][0]\n",
    "        summary = \" \".join(summary.replace(u\"\\xa0\", u\" \").strip().split())\n",
    "\n",
    "    return summary\n",
    "\n",
    "\n",
    "\n",
    "def doc2vec_summarizer(text,num_sen=5):\n",
    "\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "\n",
    "    LabeledSentence = gensim.models.doc2vec.LabeledSentence\n",
    "\n",
    "    sentences = doc2vec_tokenizeText(sentenceTokens)\n",
    "\n",
    "    def labelizeReviews(reviews, label_type):\n",
    "        labelized = []\n",
    "        for i,v in enumerate(reviews):\n",
    "            label = '%s_%s'%(label_type,i)\n",
    "            labelized.append(LabeledSentence(v, [label]))\n",
    "        return labelized\n",
    "\n",
    "    sentences = labelizeReviews(sentences,'train')\n",
    "\n",
    "    model = Doc2Vec(min_count=1, window=10, size=500, sample=1e-4, workers=8)\n",
    "    model.build_vocab(sentences)\n",
    "    for epoch in range(1000):\n",
    "        model.train(sentences)\n",
    "\n",
    "\n",
    "    docvec = []\n",
    "    for i in range(len(model.docvecs)):\n",
    "        docvec.append(model.docvecs[i])\n",
    "\n",
    "    kmeans = KMeans(n_clusters=1)\n",
    "\n",
    "    kmeans.fit(docvec)\n",
    "\n",
    "    distance = pairwise_distances(kmeans.cluster_centers_, docvec)\n",
    "\n",
    "    pos = np.array(list(range(len(sentenceTokens))))\n",
    "\n",
    "    listlist = [list(x) for x in zip(sentenceTokens,distance.tolist()[0],pos)]\n",
    "\n",
    "    listlist.sort(key=lambda x: x[1],reverse=False)\n",
    "\n",
    "    ## Sort by sentence order\n",
    "    summarysentences = listlist[0:num_sen]\n",
    "\n",
    "    summarysentences.sort(key=lambda x: x[2],reverse=False)\n",
    "\n",
    "    summary = \"\"\n",
    "    for n in range(num_sen):\n",
    "        summary += ' ' + summarysentences[n][0]\n",
    "        summary = \" \".join(summary.replace(u\"\\xa0\", u\" \").strip().split())\n",
    "\n",
    "    return summary\n",
    "        \n",
    "def lsa_summarizer(text,num_sen=5):\n",
    "    sent_detector = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "    sentenceTokens = sent_detector.tokenize(text.strip())\n",
    "\n",
    "    tfvectorizer = TfidfVectorizer(tokenizer=tokenizeText)\n",
    "    sparse = tfvectorizer.fit_transform(sentenceTokens).A\n",
    "    lsa = TruncatedSVD(n_components=1)\n",
    "    concept = lsa.fit_transform(sparse)\n",
    "\n",
    "    pos = np.array(list(range(len(sentenceTokens))))    \n",
    "    \n",
    "    listlist = [list(x) for x in zip(sentenceTokens,concept,pos)]\n",
    "\n",
    "    listlist.sort(key=lambda x: x[1],reverse=True)\n",
    "\n",
    "    summarysentences = listlist[0:num_sen]\n",
    "\n",
    "    summarysentences.sort(key=lambda x: x[2],reverse=False)\n",
    "\n",
    "    summary = \"\"\n",
    "    for n in range(num_sen):\n",
    "        summary += ' ' + summarysentences[n][0]\n",
    "        summary = \" \".join(summary.replace(u\"\\xa0\", u\" \").strip().split())\n",
    "\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "experitext = \"The boy pet the dog. Boy petted the dog. The dog got petted boy. Dog said woof woof. Boy said come here boy. Dog did not go to boy. Cat is fat. Bear is big. Pig is cute. The cavs won the game. Warriors still have a chance. Penguins escaped the sharks!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The boy pet the dog. Boy petted the dog. The dog got petted boy. Dog said woof woof. Boy said come here boy. Dog did not go to boy. Cat is fat. Bear is big. Pig is cute. The cavs won the game. Warriors still have a chance. Penguins escaped the sharks!\n"
     ]
    }
   ],
   "source": [
    "print(experitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node The cavs won the game.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Cat is fat.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Pig is cute.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Warriors still have a chance.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Bear is big.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Penguins escaped the sharks!\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The boy pet the dog. Boy petted the dog. The dog got petted boy. Boy said come here boy. Dog did not go to boy.'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_summarizer(experitext, weight='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node The cavs won the game.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Cat is fat.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Pig is cute.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Warriors still have a chance.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Bear is big.\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n",
      "C:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\networkx\\generators\\stochastic.py:56: UserWarning: zero out-degree for node Penguins escaped the sharks!\n",
      "  warnings.warn('zero out-degree for node %s' % u)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'The boy pet the dog. Boy petted the dog. The dog got petted boy. Boy said come here boy. Dog did not go to boy.'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_summarizer(experitext, weight='similarity')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The boy pet the dog. Boy petted the dog. The dog got petted boy. Boy said come here boy. Dog did not go to boy.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_summarizer(experitext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The boy pet the dog. Boy petted the dog. The dog got petted boy. Dog said woof woof. Pig is cute.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_summarizer(experitext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.0 Sample runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sample Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 364,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "text2 = '\\n\\nBRUSSELS – The “Brexiteers” – those who want Britain to leave the European Union – argue that their goal would be virtually cost-free and have no effect on the United Kingdom’s global trade. They are wrong. On June 23, when voters in Britain cast their ballots in the referendum on the question, they need to consider what is actually involved in leaving the EU – and how the free-trade benefits they now enjoy (and take for granted) could be maintained after Brexit. Start with the basics. Leaving the EU means that the UK would exit the EU’s Customs Union, which is the basis for cross-border free trade among the EU’s 28 members (and establishes a common external tariff vis-à-vis third countries). It also means exit from the Single Market – the basis for the free movement of goods and services among EU members. By definition, non-members of the EU cannot belong to the Single Market.\\n\\nWill Brexit Destroy Britain and Europe?\\n\\nPhilippe Legrain weighs the views of Joschka Fischer, Richard Haass, Joseph Nye, and others on what Britain’s withdrawal from the EU would mean for both sides.\\n\\nSo what would happen next? During the two-year period before Britain’s withdrawal takes final effect, there would be UK-EU negotiations on many points – sovereignty, the legal order, immigration, finances, and economic matters. The assumption is that a crucial goal for Britain would be to negotiate a trading relationship as close as possible to the free-trade relationships that exist today.That is easier said than done. The best result would be if all players agreed to maintain the free trade already achieved, with the UK setting a new external tariff on a duty-free basis, applicable to all comers. This is what happened in the 1970s after Britain and Denmark left the European Free Trade Association: Free-trade agreements were negotiated among EFTA members and between them and the EU (or the EEC as it was then known). But Brexiteers should realize that there is no guarantee that this would happen again – and, in any case, there would be complications. While this solution would be good for the 45% of British exports that are sold in EU markets, it would reduce protection for British industries to zero. Under the rules of the World Trade Organization, the same import duties must be applied to all WTO participants – which means that if Britain’s imports from the EU are duty-free, its imports from the rest of the world must be, too. The alternative would be for UK exporters to accept the EU’s common external tariff, and for the UK to create its own import tariff, applied to all imports, including from the EU. Because the common tariff is at a relatively low level on industrial and fishery products, this might not be an insuperable barrier for UK exports, and it would allow some flexibility in protecting UK companies from imports. The potential pitfall is that any British tariff increase above the EU level would expose the UK to claims for compensation from third countries in the WTO. The bigger question that the Brexiteers need to answer is how to secure a high level of access to the EU’s internal market. This is vitally important for Britain’s service industries, particularly for the City of London’s exports of financial services. There is only one precedent for non-EU members being able to negotiate access to the internal market equivalent to that enjoyed by EU members. This is the European Economic Area agreement concluded with the EU by Norway, Iceland, and Liechtenstein in 1992.In the view of many observers, including me, access to the Single Market through the EEA is no longer available. But what if we’re wrong? The point is that such a deal would go against all the instincts (and rhetoric) of the Brexiteers, because it would mean accepting the EU’s “four freedoms”: not just the free movement of goods, services, and capital – but of people, too. That would be hard to square with the Brexit objective of “controlling our borders.” The Brexiteers would also blanch at Britain’s obligation under an EEA-type agreement to continue contributing to the EU budget.Of course, there could be specific arrangements for particular sectors. But it seems unlikely that such arrangements would be possible in financial services and the major professional services (including doctors, architects, and lawyers), which are important for Britain’s competitors in Europe. Indeed, it is possible that the EU would adopt a tough mercantilist stance: If you want privileged access, you should stay in the club. The final consequence of Brexit is that the UK would lose its free-trade arrangements with third countries under the many trade agreements that the EU has signed since 2000. Replacing these agreements with bilateral deals would take time. There is no guarantee that the EU would agree to an interim continuation of free trade, and it seems certain that UK exports would face higher tariffs than its former EU partners in those third countries (placing British exporters at a competitive disadvantage).\\n\\nIt’s a big stretch for the Brexiteers to ask us to believe that the world’s most prominent leaders, European or not, have all got the impact of Brexit wrong. In the trade debate, we have had the International Monetary Fund, US President Barack Obama, and the OECD – quite apart from the UK Treasury – telling Britons that a vote to leave would be bad for the economy. It is not much of a reply to argue that the OECD is “in the pay of the EU” or that Obama is anti-British because he had a Kenyan father in British colonial days.Economic forecasting is an uncertain science. But when almost all projections point in the same direction – that Brexit would be hugely damaging to the UK – it is time to decide what is credible and what is not.\\n\\n'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UserWarning: zero out-degree for node Start with the basics. [stochastic.py:56]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Leaving the EU means that the UK would exit the EU’s Customs Union, which is the basis for cross-border free trade among the EU’s 28 members (and establishes a common external tariff vis-à-vis third countries).It also means exit from the Single Market – the basis for the free movement of goods and services among EU members.This is what happened in the 1970s after Britain and Denmark left the European Free Trade Association: Free-trade agreements were negotiated among EFTA members and between them and the EU (or the EEC as it was then known).The alternative would be for UK exporters to accept the EU’s common external tariff, and for the UK to create its own import tariff, applied to all imports, including from the EU.There is no guarantee that the EU would agree to an interim continuation of free trade, and it seems certain that UK exports would face higher tariffs than its former EU partners in those third countries (placing British exporters at a competitive disadvantage).'"
      ]
     },
     "execution_count": 428,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "textrank_summarizer(text2,weight='cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 384,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Leaving the EU means that the UK would exit the EU’s Customs Union, which is the basis for cross-border free trade among the EU’s 28 members (and establishes a common external tariff vis-à-vis third countries).It also means exit from the Single Market – the basis for the free movement of goods and services among EU members.This is what happened in the 1970s after Britain and Denmark left the European Free Trade Association: Free-trade agreements were negotiated among EFTA members and between them and the EU (or the EEC as it was then known).The alternative would be for UK exporters to accept the EU’s common external tariff, and for the UK to create its own import tariff, applied to all imports, including from the EU.The point is that such a deal would go against all the instincts (and rhetoric) of the Brexiteers, because it would mean accepting the EU’s “four freedoms”: not just the free movement of goods, services, and capital – but of people, too.'"
      ]
     },
     "execution_count": 384,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsa_summarizer(text2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'On June 23, when voters in Britain cast their ballots in the referendum on the question, they need to consider what is actually involved in leaving the EU – and how the free-trade benefits they now enjoy (and take for granted) could be maintained after Brexit.The assumption is that a crucial goal for Britain would be to negotiate a trading relationship as close as possible to the free-trade relationships that exist today.That is easier said than done.The point is that such a deal would go against all the instincts (and rhetoric) of the Brexiteers, because it would mean accepting the EU’s “four freedoms”: not just the free movement of goods, services, and capital – but of people, too.That would be hard to square with the Brexit objective of “controlling our borders.” The Brexiteers would also blanch at Britain’s obligation under an EEA-type agreement to continue contributing to the EU budget.Of course, there could be specific arrangements for particular sectors.There is no guarantee that the EU would agree to an interim continuation of free trade, and it seems certain that UK exports would face higher tariffs than its former EU partners in those third countries (placing British exporters at a competitive disadvantage).'"
      ]
     },
     "execution_count": 378,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc2vec_summarizer(text2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "import PyPDF2\n",
    "\n",
    "from PyPDF2 import PdfFileWriter, PdfFileReader\n",
    "\n",
    "input1 = PdfFileReader(open(\"C:/Users/kennd/Downloads/documentation.pdf\", \"rb\"))\n",
    "\n",
    "pages = input1.getNumPages()\n",
    "\n",
    "text = \"\"\n",
    "for i in range(pages):\n",
    "    text += input1.getPage(i).extractText()\n",
    "    text = \" \".join(text.replace(u\"\\xa0\", u\" \").strip().split())\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 2] The system cannot find the file specified",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-8-3e6ccc37364e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     31\u001b[0m         \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mMath\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'='\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mMatrix\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mM\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     32\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 33\u001b[1;33m \u001b[0mdoc\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_pdf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'full'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcompiler\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'pdflatex'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\pylatex\\document.py\u001b[0m in \u001b[0;36mgenerate_pdf\u001b[1;34m(self, filepath, clean, clean_tex, compiler, compiler_args, silent)\u001b[0m\n\u001b[0;32m    201\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    202\u001b[0m             \u001b[1;31m# If none of the compilers worked, raise the last error\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 203\u001b[1;33m             \u001b[1;32mraise\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos_error\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    204\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    205\u001b[0m         \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mchdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcur_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\site-packages\\pylatex\\document.py\u001b[0m in \u001b[0;36mgenerate_pdf\u001b[1;34m(self, filepath, clean, clean_tex, compiler, compiler_args, silent)\u001b[0m\n\u001b[0;32m    155\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    156\u001b[0m                 output = subprocess.check_output(command,\n\u001b[1;32m--> 157\u001b[1;33m                                                  stderr=subprocess.STDOUT)\n\u001b[0m\u001b[0;32m    158\u001b[0m             \u001b[1;32mexcept\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mOSError\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIOError\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    159\u001b[0m                 \u001b[1;31m# Use FileNotFoundError when python 2 is dropped\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mcheck_output\u001b[1;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    627\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    628\u001b[0m     return run(*popenargs, stdout=PIPE, timeout=timeout, check=True,\n\u001b[1;32m--> 629\u001b[1;33m                **kwargs).stdout\n\u001b[0m\u001b[0;32m    630\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    631\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(input, timeout, check, *popenargs, **kwargs)\u001b[0m\n\u001b[0;32m    694\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'stdin'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mPIPE\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    695\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 696\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    697\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mstdout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstderr\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprocess\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcommunicate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds)\u001b[0m\n\u001b[0;32m    948\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    949\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 950\u001b[1;33m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[0;32m    951\u001b[0m         \u001b[1;32mexcept\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    952\u001b[0m             \u001b[1;31m# Cleanup if the child failed starting.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mC:\\Users\\kennd\\Anaconda3\\lib\\subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[1;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, unused_restore_signals, unused_start_new_session)\u001b[0m\n\u001b[0;32m   1218\u001b[0m                                          \u001b[0menv\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1219\u001b[0m                                          \u001b[0mcwd\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1220\u001b[1;33m                                          startupinfo)\n\u001b[0m\u001b[0;32m   1221\u001b[0m             \u001b[1;32mfinally\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1222\u001b[0m                 \u001b[1;31m# Child is launched. Close the parent's copy of those pipe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 2] The system cannot find the file specified"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from pylatex import Document, Section, Subsection, Tabular, Math, TikZ, Axis,Plot, Figure, Package, Matrix\n",
    "from pylatex.utils import italic\n",
    "import os\n",
    "\n",
    "doc = Document()\n",
    "\n",
    "with doc.create(Section('The simple stuff')):\n",
    "    doc.append('Some regular text and some')\n",
    "    doc.append(italic('italic text. '))\n",
    "    doc.append('\\nAlso some crazy characters: $&#{}')\n",
    "    with doc.create(Subsection('Math that is incorrect')):\n",
    "        doc.append(Math(data=['2*3', '=', 9]))\n",
    "\n",
    "    with doc.create(Subsection('Table of something')):\n",
    "        with doc.create(Tabular('rc|cl')) as table:\n",
    "            table.add_hline()\n",
    "            table.add_row((1, 2, 3, 4))\n",
    "            table.add_hline(1, 2)\n",
    "            table.add_empty_row()\n",
    "            table.add_row((4, 5, 6, 7))\n",
    "\n",
    "a = np.array([[100, 10, 20]]).T\n",
    "M = np.matrix([[2, 3, 4],\n",
    "               [0, 0, 1],\n",
    "               [0, 0, 2]])\n",
    "\n",
    "with doc.create(Section('The fancy stuff')):\n",
    "    with doc.create(Subsection('Correct matrix equations')):\n",
    "        doc.append(Math(data=[Matrix(M), Matrix(a), '=', Matrix(M * a)]))\n",
    "                \n",
    "doc.generate_pdf('full',compiler='pdflatex')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
